# Redis-Cpp Benchmarking and Comparison

This directory contains the exploration code, benchmark suite, and results for the [redis-cpp](https://github.com/tdv/redis-cpp) library, as part of a broader comparison of Redis C++17 libraries against [redis-plus-plus](https://github.com/sewenew/redis-plus-plus). The goal is to evaluate performance, usability, and other key aspects to determine suitability for specific use cases.

## Setup and Exploration Code
- **Basic Examples**: Simple programs to connect to Redis and perform basic operations are located in the `test/src` directory. These examples help in understanding the API and usage patterns of redis-cpp.
- **Build Configuration**: The project uses CMake for build management, ensuring C++17 standard compliance with compilers like GCC or Clang. Instructions for compilation and execution are provided below.
- **Docker Environment**: A Dockerfile is included for containerized build and execution, ensuring a consistent testing environment. See the Dockerfile for setup details.

### Compilation and Execution
1. Ensure you have CMake and a C++17 compatible compiler installed.
2. Clone the repository and navigate to this directory.
3. Run the following commands to build the exploration code:
   ```
   mkdir build
   cd build
   cmake ..
   make
   ```
4. Execute the basic example:
   ```
   ./test/redis_cpp_test
   ```
   (Ensure a Redis server is running locally or specify connection details in the code.)

### Docker Setup
To use the Docker environment for testing:
1. Build the Docker image:
   ```
   docker build -t redis-cpp-benchmark .
   ```
2. Run the container with access to the Redis server (adjust host and port as needed):
   ```
   docker run -it --rm -p 6379:6379 redis-cpp-benchmark
   ```

## Benchmark Suite
A comprehensive benchmark suite using Google Benchmark has been implemented for redis-cpp, covering the 20 most common Redis operations:
- SET, GET, DEL, INCR, EXISTS, EXPIRE, TTL, DECR, HSET, HGET, HMSET, HMGET, LPUSH, LPOP, RPUSH, RPOP, SADD, SMEMBERS, SCARD, SISMEMBER

### Benchmark Implementation
- **Source Files**: Located in `benchmark/src/`, with individual files for each operation (e.g., `set_benchmark.cpp`, `get_benchmark.cpp`).
- **Build Instructions**: Use the Makefile or CMake configuration in the `benchmark` directory to build the benchmarks.
  ```
  cd benchmark
  mkdir build
  cd build
  cmake ..
  make
  ```
- **Execution**: Run the benchmark executable to generate results. Results are saved in the `results` directory as CSV files (e.g., `redis_cpp_results_run_1.csv`).

### Benchmark Results
- **Results Directory**: `results/` contains raw benchmark output files from multiple runs to account for variability.
- **Processed Averages**: A summary CSV file (`redis_cpp_average.csv`) is generated by processing the raw results with a Python script. This file includes per-operation averages for real_time, cpu_time, and iterations, as well as an overall average across all operations.
- **Processing Results**: Use the script located at `../../tools/benchmark/calculate_averages.py` to process results:
  ```
  python ../../tools/benchmark/calculate_averages.py results
  ```

## Performance Analysis
[Placeholder for detailed performance analysis once finalized. This section will include tables or graphs comparing throughput and latency with redis-plus-plus for individual operations and overall performance.]

## Qualitative Analysis
The library will be evaluated based on the following criteria:
- **Ease of Use**: API design, intuitiveness, and learning curve.
- **Features**: Support for Redis features (e.g., transactions, pub/sub, clustering).
- **Documentation**: Quality and completeness of official documentation.
- **Community Support**: Activity on GitHub, issue resolution, and updates.
- **Compatibility**: Support for C++17 standards and integration with modern C++ practices.

[Placeholder for detailed subjective analysis based on the above criteria.]

## Next Steps
- Finalize the analysis of benchmark data to compare performance metrics with redis-plus-plus.
- Visualize results for clarity in documentation.
- Complete the qualitative analysis based on the comparison criteria.
- Update this README with detailed findings and conclusions.
